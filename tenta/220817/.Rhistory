# a = length(post_sample_beta[,1])
# a
for(i in 1:length(post_sample_beta[,1]))
{
if((post_sample_beta[i,2] > 0)  & (post_sample_beta[i,3] > 0))
{
yes = yes + 1
}
else
{
no = no + 1
}
}
print(yes/(yes+no))
x = as.matrix(c(1,5,1))
pred = post_sample_beta %*% x
p_i = exp(pred) / (1 +exp(pred))
task_calc = (1-p_i) / p_i
plot(density(task_calc))
min(X[,2])
# Yes it is very reasonable with large odds that this bridge was built very
# recently. Yes, we should question the results because compared to the youngest
# bridge of 19 year in the data.
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
d_calc = c()
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
d_calc[i] = post_sample_beta %*% new_x
}
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
d_calc = c()
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
d_calc[i] = post_sample_beta %*% new_x
}
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
d_calc = c()
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
d_calc[i] = post_sample_beta %*% new_x
}
knitr::opts_chunk$set(echo = TRUE,include=FALSE, warnings = FALSE)
knitr::opts_chunk$set(echo = TRUE,include=FALSE, warnings = FALSE)
n = 4
sumone_xi = (1/0.7) + (1/1.1) + (1/0.9) + (1/1.5)
# sumx4 = log(0.7**4) + log(1.1**4) + log(0.9**4) + log(1.5**4)
logpost <- function(theta, sumone_xi)
{
logprior = 3 * log(theta) - 2*log(theta)
loglik = 12*log(theta) - (theta * sumone_xi)
return (loglik + logprior)
}
theta_grid = seq(from=0.01, to=9.0, by = 0.01)
res = c()
for (i in 1:length(theta_grid))
{
res[i] = logpost(theta_grid[i], sumone_xi)
}
res = exp(res)
res_normalized = res / (0.01 * sum(res))
plot(x=theta_grid, y = res_normalized)
OptimRes <- optim(0.5,logpost,gr=NULL,sumone_xi, lower=0.1, method = c("L-BFGS-B"),control=list(fnscale=-1),hessian=TRUE)
approx = dnorm(theta_grid, OptimRes$par, sqrt(-1/OptimRes$hessian))
plot(x=theta_grid, y = res_normalized)
lines(theta_grid, approx, col = "red")
# The normal approximation is skewed to the left
library(mvtnorm)
BayesLogitReg <- function(y, X, mu_0, Sigma_0, nIter){
# Sampling from a logistic regression with prior:
#
# beta ~ N(mu_0, Sigma_0)
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Sigma_0  - prior covariance matrix for beta
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
LogPostLogistic <- function(betas,y,X,mu_0,Sigma_0){
linPred <- X%*%betas;
logLik <- sum( linPred*y - log(1 + exp(linPred)) );
if (abs(logLik) == Inf){
logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
}
logPrior <- dmvnorm(betas, mu_0, Sigma_0, log=TRUE);
return(logLik + logPrior)
}
Npar <- dim(X)[2]
initVal <- matrix(0,Npar,1)
OptimRes <- optim(initVal,LogPostLogistic,gr=NULL,y,X,mu_0,Sigma_0,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
mu_n <- OptimRes$par
Sigma_n <- -solve(OptimRes$hessian)
betaSample <- rmvnorm(nIter,mu_n,Sigma_n)
return(results = list(betaSample=betaSample))
}
load(file = 'Bridge.RData')
# INPUTS:
# CHECK   y - n-by-1 vector with response data observations
# CHECK   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Sigma_0  - prior covariance matrix for beta
#   nIter - Number of samples from the posterior (iterations)
mu_0 = matrix(0,3,1)
Sigma_0 = diag(10**2,3,3)
nIter = 20000
results = BayesLogitReg(y,X,mu_0,Sigma_0,nIter)
post_sample_beta = results$betaSample
q = quantile(post_sample_beta[,2], prob=c(0.025, 0.975))
print("There is a 95% probability that b1 is within the interval A and B")
print( q[1])
print( q[2])
yes = 0
no = 0
# a = length(post_sample_beta[,1])
# a
for(i in 1:length(post_sample_beta[,1]))
{
if((post_sample_beta[i,2] > 0)  & (post_sample_beta[i,3] > 0))
{
yes = yes + 1
}
else
{
no = no + 1
}
}
print(yes/(yes+no))
x = as.matrix(c(1,5,1))
pred = post_sample_beta %*% x
p_i = exp(pred) / (1 +exp(pred))
task_calc = (1-p_i) / p_i
plot(density(task_calc))
min(X[,2])
# Yes it is very reasonable with large odds that this bridge was built very
# recently. Yes, we should question the results because compared to the youngest
# bridge of 19 year in the data.
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
d_calc = c()
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
d_calc[i] = post_sample_beta %*% new_x
}
knitr::opts_chunk$set(echo = TRUE,include=FALSE, warning = FALSE)
n = 4
sumone_xi = (1/0.7) + (1/1.1) + (1/0.9) + (1/1.5)
# sumx4 = log(0.7**4) + log(1.1**4) + log(0.9**4) + log(1.5**4)
logpost <- function(theta, sumone_xi)
{
logprior = 3 * log(theta) - 2*log(theta)
loglik = 12*log(theta) - (theta * sumone_xi)
return (loglik + logprior)
}
theta_grid = seq(from=0.01, to=9.0, by = 0.01)
res = c()
for (i in 1:length(theta_grid))
{
res[i] = logpost(theta_grid[i], sumone_xi)
}
res = exp(res)
res_normalized = res / (0.01 * sum(res))
plot(x=theta_grid, y = res_normalized)
OptimRes <- optim(0.5,logpost,gr=NULL,sumone_xi, lower=0.1, method = c("L-BFGS-B"),control=list(fnscale=-1),hessian=TRUE)
approx = dnorm(theta_grid, OptimRes$par, sqrt(-1/OptimRes$hessian))
plot(x=theta_grid, y = res_normalized)
lines(theta_grid, approx, col = "red")
# The normal approximation is skewed to the left
library(mvtnorm)
BayesLogitReg <- function(y, X, mu_0, Sigma_0, nIter){
# Sampling from a logistic regression with prior:
#
# beta ~ N(mu_0, Sigma_0)
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Sigma_0  - prior covariance matrix for beta
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
LogPostLogistic <- function(betas,y,X,mu_0,Sigma_0){
linPred <- X%*%betas;
logLik <- sum( linPred*y - log(1 + exp(linPred)) );
if (abs(logLik) == Inf){
logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
}
logPrior <- dmvnorm(betas, mu_0, Sigma_0, log=TRUE);
return(logLik + logPrior)
}
Npar <- dim(X)[2]
initVal <- matrix(0,Npar,1)
OptimRes <- optim(initVal,LogPostLogistic,gr=NULL,y,X,mu_0,Sigma_0,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
mu_n <- OptimRes$par
Sigma_n <- -solve(OptimRes$hessian)
betaSample <- rmvnorm(nIter,mu_n,Sigma_n)
return(results = list(betaSample=betaSample))
}
load(file = 'Bridge.RData')
# INPUTS:
# CHECK   y - n-by-1 vector with response data observations
# CHECK   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Sigma_0  - prior covariance matrix for beta
#   nIter - Number of samples from the posterior (iterations)
mu_0 = matrix(0,3,1)
Sigma_0 = diag(10**2,3,3)
nIter = 20000
results = BayesLogitReg(y,X,mu_0,Sigma_0,nIter)
post_sample_beta = results$betaSample
q = quantile(post_sample_beta[,2], prob=c(0.025, 0.975))
print("There is a 95% probability that b1 is within the interval A and B")
print( q[1])
print( q[2])
yes = 0
no = 0
# a = length(post_sample_beta[,1])
# a
for(i in 1:length(post_sample_beta[,1]))
{
if((post_sample_beta[i,2] > 0)  & (post_sample_beta[i,3] > 0))
{
yes = yes + 1
}
else
{
no = no + 1
}
}
print(yes/(yes+no))
x = as.matrix(c(1,5,1))
pred = post_sample_beta %*% x
p_i = exp(pred) / (1 +exp(pred))
task_calc = (1-p_i) / p_i
plot(density(task_calc))
min(X[,2])
# Yes it is very reasonable with large odds that this bridge was built very
# recently. Yes, we should question the results because compared to the youngest
# bridge of 19 year in the data.
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
d_calc = c()
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
d_calc[i] = post_sample_beta %*% new_x
}
knitr::opts_chunk$set(echo = TRUE,include=FALSE, warning = FALSE)
n = 4
sumone_xi = (1/0.7) + (1/1.1) + (1/0.9) + (1/1.5)
# sumx4 = log(0.7**4) + log(1.1**4) + log(0.9**4) + log(1.5**4)
logpost <- function(theta, sumone_xi)
{
logprior = 3 * log(theta) - 2*log(theta)
loglik = 12*log(theta) - (theta * sumone_xi)
return (loglik + logprior)
}
theta_grid = seq(from=0.01, to=9.0, by = 0.01)
res = c()
for (i in 1:length(theta_grid))
{
res[i] = logpost(theta_grid[i], sumone_xi)
}
res = exp(res)
res_normalized = res / (0.01 * sum(res))
plot(x=theta_grid, y = res_normalized)
OptimRes <- optim(0.5,logpost,gr=NULL,sumone_xi, lower=0.1, method = c("L-BFGS-B"),control=list(fnscale=-1),hessian=TRUE)
approx = dnorm(theta_grid, OptimRes$par, sqrt(-1/OptimRes$hessian))
plot(x=theta_grid, y = res_normalized)
lines(theta_grid, approx, col = "red")
# The normal approximation is skewed to the left
library(mvtnorm)
BayesLogitReg <- function(y, X, mu_0, Sigma_0, nIter){
# Sampling from a logistic regression with prior:
#
# beta ~ N(mu_0, Sigma_0)
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Sigma_0  - prior covariance matrix for beta
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
LogPostLogistic <- function(betas,y,X,mu_0,Sigma_0){
linPred <- X%*%betas;
logLik <- sum( linPred*y - log(1 + exp(linPred)) );
if (abs(logLik) == Inf){
logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
}
logPrior <- dmvnorm(betas, mu_0, Sigma_0, log=TRUE);
return(logLik + logPrior)
}
Npar <- dim(X)[2]
initVal <- matrix(0,Npar,1)
OptimRes <- optim(initVal,LogPostLogistic,gr=NULL,y,X,mu_0,Sigma_0,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
mu_n <- OptimRes$par
Sigma_n <- -solve(OptimRes$hessian)
betaSample <- rmvnorm(nIter,mu_n,Sigma_n)
return(results = list(betaSample=betaSample))
}
load(file = 'Bridge.RData')
# INPUTS:
# CHECK   y - n-by-1 vector with response data observations
# CHECK   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Sigma_0  - prior covariance matrix for beta
#   nIter - Number of samples from the posterior (iterations)
mu_0 = matrix(0,3,1)
Sigma_0 = diag(10**2,3,3)
nIter = 20000
results = BayesLogitReg(y,X,mu_0,Sigma_0,nIter)
post_sample_beta = results$betaSample
q = quantile(post_sample_beta[,2], prob=c(0.025, 0.975))
print("There is a 95% probability that b1 is within the interval A and B")
print( q[1])
print( q[2])
yes = 0
no = 0
# a = length(post_sample_beta[,1])
# a
for(i in 1:length(post_sample_beta[,1]))
{
if((post_sample_beta[i,2] > 0)  & (post_sample_beta[i,3] > 0))
{
yes = yes + 1
}
else
{
no = no + 1
}
}
print(yes/(yes+no))
x = as.matrix(c(1,5,1))
pred = post_sample_beta %*% x
p_i = exp(pred) / (1 +exp(pred))
task_calc = (1-p_i) / p_i
plot(density(task_calc))
min(X[,2])
# Yes it is very reasonable with large odds that this bridge was built very
# recently. Yes, we should question the results because compared to the youngest
# bridge of 19 year in the data.
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = c()
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
pi_list[i] = post_sample_beta %*% new_x
}
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = c()
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_list[i] = exp(in_par)/ (1+exp(in_par))
}
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = as.matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,] = quantile(pi_d, prob=c(0.025, 0.975))
}
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = as.matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
pi_list = as.matrix(0,length(x_grid), 2)
View(pi_list)
pi_list = as.matrix(0,length(x_grid), 2)
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
plot(x_gridm pi_list)
plot(x_grid,pi_list)
plot(x_grid,pi_list[,1])
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
plot(x_grid,pi_list[,1])
lines(x_grid,pi_list[,2])
View(pi_list)
plot(x_grid,pi_list[,1], col = "RED", type = "l")
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
plot(x_grid,pi_list[,1], col = "RED", type = "l")
lines(x_grid,pi_list[,2],col = "BLUE", type = "l"
lines(x_grid,pi_list[,2],col = "BLUE", type = "l")
plot(x_grid,pi_list[,1], col = "RED", type = "l")
lines(x_grid,pi_list[,2],col = "BLUE", type = "l")
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
plot(x_grid,pi_list[,1], col = "RED", type = "l")
lines(x_grid,pi_list[,2],col = "BLUE", type = "l")
plot(x_grid,pi_list[,2],col = "BLUE", type = "l")
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
plot(x_grid,pi_list[,1], col = "RED", type = "l")
plot(x_grid,pi_list[,2],col = "BLUE", type = "l")
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
plot(x_grid,pi_list[,1], col = "RED", type = "l",ylim = c(0,3))
lines(x_grid,pi_list[,2],col = "BLUE", type = "l")
x_grid = seq(from = min(X[,2]), to = max(X[,2]), by = 0.1)
pi_list = matrix(0,length(x_grid), 2)
for (i in 1:length(x_grid))
{
new_x = as.matrix(c(1,x_grid[i],0))
in_par = post_sample_beta %*% new_x
pi_d = exp(in_par)/ (1+exp(in_par))
pi_list[i,1:2] = quantile(pi_d, prob=c(0.025, 0.975))
}
plot(x_grid,pi_list[,1], col = "RED", type = "l",ylim = c(0,1))
lines(x_grid,pi_list[,2],col = "BLUE", type = "l")
x_e = as.matrix(c(1,40,1))
x_e = as.matrix(c(1,40,1))
inside_par = x_e %*% post_sample_beta
x_e = as.matrix(c(1,40,1))
inside_par =  post_sample_beta %*% x_e
x_e = as.matrix(c(1,40,1))
inside_par =  post_sample_beta %*% x_e
p_i = exp(inside_par) / (1 + exp(inside_par))
x_e = as.matrix(c(1,40,1))
inside_par =  post_sample_beta %*% x_e
p_i = exp(inside_par) / (1 + exp(inside_par))
plot(density(p_i))
x_e = as.matrix(c(1,40,1))
inside_par =  post_sample_beta %*% x_e
p_i = exp(inside_par) / (1 + exp(inside_par))
plot(density(p_i))
mean(p_i > 0.5)
(60 * 0.6) - (20*0.4)
(180 * 0.6) - (240*0.4)
(60 * 0.6) - (20*0.4)
(180 * 0.6) - (240*0.4)
?beta
?bern
?bernoulli
??bern
6 + 32 + 50
38/60
theta = 38/60
theta = 38/60
EU_buy = theta * 60 - (20*(1-theta))
EU_dont = theta * 180 - (240*(1-theta))
print(EU_buy)
print(EU_dont)
